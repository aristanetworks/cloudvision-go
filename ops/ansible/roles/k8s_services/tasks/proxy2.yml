# Copyright (c) 2017 Arista Networks, Inc.  All rights reserved.
# Arista Networks, Inc. Confidential and Proprietary.
# Subject to Arista Networks, Inc.'s EULA.
# FOR INTERNAL USE ONLY. NOT FOR DISTRIBUTION.

- name: add nginx catchall configmap
  kubernetes:
    api_endpoint: 127.0.0.1:8001
    state: apply
    insecure: true
    inline_data: |
      apiVersion: v1
      kind: ConfigMap
      metadata:
        name: nginx-catchall-config
      data:
        nginx.conf: |
          # Copyright (c) 2015 Arista Networks, Inc.  All rights reserved.
          # Arista Networks, Inc. Confidential and Proprietary.
          user  nginx;
          worker_processes  1;

          error_log  stderr warn;
          pid        /var/run/nginx.pid;

          events {
              worker_connections  1024;
          }

          http {
              default_type  text/plain;

              log_format  main  '$remote_addr $host - $remote_user [$time_local] "$request" '
                                '$status $body_bytes_sent "$http_referer" '
                                '"$http_user_agent" "$http_x_forwarded_for"';

              access_log  stdout main;

              keepalive_timeout  65;

              server {
                  listen       8080;
                  server_name  localhost;
                  server_tokens off;

                  location / {
                      return 404 "404 Not Found";
                  }
              }
          }

# https proxy. Exposes port 443 only.
- name: Add nghttpx public and private https config
  with_dict: "{{ services }}"
  kubernetes:
    api_endpoint: 127.0.0.1:8001
    state: apply
    insecure: true
    inline_data: |
      #jinja2: block_start_string: '[%', block_end_string: '%]', variable_start_string: '[[', variable_end_string: ']]'
      apiVersion: v1
      kind: ConfigMap
      metadata:
        name: nghttpx-conf-https-[[ item.key ]]
      data:
        nghttpx.conf.tmpl: |
          # Copyright (c) 2017 Arista Networks, Inc.  All rights reserved.
          # Arista Networks, Inc. Confidential and Proprietary.
          # Subject to Arista Networks, Inc.'s EULA.
          # FOR INTERNAL USE ONLY. NOT FOR DISTRIBUTION.

          no-via=yes
          add-x-forwarded-for=yes
          server-name=Arista

          # frontend HTTPS
          # TODO: Bind to the correct interface
          frontend=*,443

          # frontend HTTP for nghttpx api access
          # Must ALWAYS be a unix socket local to the pod. NEVER expose.
          # Disabled for now
          # frontend=unix:/var/sharepod/nghttpxapi/socket.sock,80;no-tls;api

          # Let's use ingest cert for now as default cert
          # TODO: Change to www as default cert probably
          private-key-file=/etc/letsencrypt/live/ingest/privkey.pem
          certificate-file=/etc/letsencrypt/live/ingest/fullchain.pem

          # Catch-all backend
          ###################
          backend=127.0.0.1,8080

          # TODO
          # Must define a HTTP/1.1 service answering Let's Encrypt challenges: '<host>,<port>'

          # Exposed GRPC and HTTP services
          ################################
      [% for name in item.value %]
          # Service [[name]]
          {{- if exists "/endpoints/default/[[name]]"}}
          subcert=/etc/letsencrypt/live/[[name]]/privkey.pem:/etc/letsencrypt/live/[[name]]/fullchain.pem
              {{- range $i, $subset := (json (getv "/endpoints/default/[[name]]")).subsets }}
              {{- range $j, $addr := $subset.addresses }}
              {{- range $k, $port := $subset.ports }}
          backend={{$addr.ip}},{{.port}};[[name]].[[k8s_domain]][% if item.value[name] == "grpc" %];proto=h2[% endif %]
              {{- end }}
              {{- end }}
              {{- end }}
          {{- end }}
      [% endfor %]

- name: deploy public and private proxy2 daemonset for https
  with_dict: "{{ services }}"
  kubernetes:
    api_endpoint: 127.0.0.1:8001
    insecure: true
    state: apply
    inline_data: |
      apiVersion: extensions/v1beta1
      kind: DaemonSet
      metadata:
        name: proxy2-{{ item.key }}
      spec:
        selector:
          matchLabels:
            app: proxy2-{{ item.key }}
        template:
          metadata:
            name: proxy2-{{ item.key }}
            # annotations:
            #   TODO: enable some kind of metric to monitor nghttpx
            #   prometheus.io/scrape: "true"
            #   prometheus.io/port: "9201"
            labels:
              app: proxy2-{{ item.key }}
          spec:
            nodeSelector:
              proxy: {{ item.key }}
      {% if item.key == "public" or cluster_name in ['dev', 'staging'] %}
            hostNetwork: true
      {% endif %}
            containers:
            - name: proxy2
              image: registry.docker.sjc.aristanetworks.com:5000/k8s/proxy2
              imagePullPolicy: Always
              args:
                - -backend
                - kubernetes
              ports:
                - name: https
                  containerPort: 443
              volumeMounts:
              - name: nghttpx-conf-https-{{ item.key }}
                mountPath: /etc/confd/templates/nghttpx.conf.tmpl
                subPath: nghttpx.conf.tmpl
      {% for name in item.value %}
              - name: lecert-{{ name }}
                mountPath: /etc/letsencrypt/live/{{ name }}
      {% endfor %}
            - name: catchall-service
              image: nginx:1.13.5
              volumeMounts:
              - name: nginx-catchall-config
                mountPath: /etc/nginx/nginx.conf
                subPath: nginx.conf
            volumes:
            - name: nginx-catchall-config
              configMap:
                name: nginx-catchall-config
            - name: nghttpx-conf-https-{{ item.key }}
              configMap:
                name: nghttpx-conf-https-{{ item.key }}
      {% for name in item.value %}
            - name: lecert-{{ name }}
              secret:
                secretName: lecert-{{ name }}.{{ k8s_domain }}
      {% endfor %}

# TODO: Migrate to https
- name: Add internal http proxy config
  when: cluster_name in ['dev', 'staging']
  kubernetes:
    api_endpoint: 127.0.0.1:8001
    state: apply
    insecure: true
    inline_data: |
      #jinja2: block_start_string: '[%', block_end_string: '%]', variable_start_string: '[[', variable_end_string: ']]'
      apiVersion: v1
      kind: ConfigMap
      metadata:
        name: haproxy-conf-internal-http
      data:
        haproxy.toml: |
          [template]
          src = "haproxy.cfg.tmpl"
          dest = "/etc/haproxy/haproxy.cfg"
          keys = [
            "/services/default",
            "/endpoints/default",
            "/statefulsets/default",
          ]

          check_cmd = "/usr/sbin/haproxy -c -f {{ .src }}"
          reload_cmd = "haproxy -f /etc/haproxy/haproxy.cfg -p /var/run/haproxy.pid -D -sf $(cat /var/run/haproxy.pid)"

        haproxy.cfg.tmpl: |
          # Copyright (c) 2015 Arista Networks, Inc.  All rights reserved.
          # Arista Networks, Inc. Confidential and Proprietary.
          # Subject to Arista Networks, Inc.'s EULA.
          # FOR INTERNAL USE ONLY. NOT FOR DISTRIBUTION.

          global
            daemon
            pidfile /var/run/haproxy.pid
            maxconn 128000

          defaults
            mode http
            timeout connect 5000ms
            timeout client 50000ms
            timeout client-fin 50000ms
            timeout server 50000ms
            timeout tunnel 1h    # timeout to use with WebSocket and CONNECT
            stats enable
            stats uri /haproxyStats
            errorfile 400 /etc/haproxy/errors/400.http
            errorfile 403 /etc/haproxy/errors/403.http
            errorfile 408 /etc/haproxy/errors/408.http
            errorfile 500 /etc/haproxy/errors/500.http
            errorfile 502 /etc/haproxy/errors/502.http
            errorfile 503 /etc/haproxy/errors/503.http
            errorfile 504 /etc/haproxy/errors/504.http

          frontend httpfrontend
            # Frontend bound on all network interfaces on port 443
            bind *:80

          # HTTP Services
          ###############
          {{ range gets "/services/default/*" }}
          {{ $service := json .Value }}
          {{- if and ($service.metadata.annotations) ($service.metadata.annotations.external_type) }}
          {{- if eq $service.metadata.annotations.external_type "http" }}
            {{- $hostname := print $service.metadata.labels.app ".[[ k8s_domain ]]" }}
            {{- if $service.metadata.annotations.external_name }}
            acl host_{{$service.metadata.labels.app}} hdr(host) {{ $service.metadata.annotations.external_name }}[[ k8s_domain ]]
            {{- else if $service.metadata.annotations.external_host }}
            acl host_{{$service.metadata.labels.app}} hdr(host) {{ $service.metadata.annotations.external_host }}
            {{- else }}
            acl host_{{$service.metadata.labels.app}} hdr(host) {{ $hostname }}
            {{- end }}
            use_backend http_backend_{{$service.metadata.labels.app}} if host_{{$service.metadata.labels.app}}
          {{- end }}
          {{- end }}
          {{- end }}

          # HTTP Services from statefulsets
          #################################
          {{ range gets "/statefulsets/default/*" }}
          {{ $statefulset := json .Value }}
          {{- if and ($statefulset.metadata.annotations) ($statefulset.metadata.annotations.external_type) }}
          {{- if eq $statefulset.metadata.annotations.external_type "http" }}
            {{ $endpoints := print "/endpoints/default/" $statefulset.spec.serviceName }}
            {{- range $i, $subset := (json (getv $endpoints)).subsets }}
            {{- range $j, $addr := $subset.addresses }}
            {{- $name := split $addr.hostname "-" }}
            {{- $name := index $name 0 }}
            {{- if eq $statefulset.metadata.name $name }}
            acl host_{{$addr.hostname}}_{{$statefulset.spec.serviceName}} hdr(host) {{$addr.hostname}}.{{$statefulset.spec.serviceName}}[[ k8s_domain ]]
            use_backend http_backend_{{$addr.hostname}}_{{$statefulset.spec.serviceName}} if host_{{$addr.hostname}}_{{$statefulset.spec.serviceName}}
            {{- end }}
            {{- end }}
            {{- end }}
          {{- end }}
          {{- end }}
          {{- end }}

          # GRPC Services
          ##############
          {{- range gets "/services/default/*" }}
          {{- $service := json .Value }}
          {{- if and ($service.metadata.annotations) ($service.metadata.annotations.external_type) }}
          {{- if eq $service.metadata.annotations.external_type "grpc" }}
          {{- $extport := $service.metadata.annotations.external_port }}
          {{- $name := $service.metadata.name }}
          listen tcp_{{ $name }}
            bind *:{{ $extport }}
            mode tcp
            timeout connect  4000
            timeout client   180000
            timeout server   180000
            {{- $endpoints := print "/endpoints/default/" $name }}
            {{- range $i, $subset := (json (getv $endpoints)).subsets }}
            {{- range $j, $addr := $subset.addresses }}
            {{- range $k, $port := $subset.ports }}
            server tcp_{{ $name }}{{ print "_" $i "_" $j "_" $k }} {{$addr.ip}}:{{.port}}
            {{- end }}
            {{- end }}
            {{- end }}
          {{- end }}
          {{- end }}
          {{- end }}

          {{define "httpservice"}}
          # HTTP service
          backend http_backend_{{.metadata.labels.app}}{{ $name := .metadata.name }}
            balance roundrobin
            option httpclose
            option forwardfor
            {{ $endpoints := print "/endpoints/default/" .metadata.name }}
            {{- range $i, $subset := (json (getv $endpoints)).subsets }}
            {{- range $j, $addr := $subset.addresses }}
            {{- range $k, $port := $subset.ports }}
            server {{ $name }}{{ print "_" $i "_" $j "_" $k }} {{$addr.ip}}:{{.port}}
            {{- end }}
            {{- end }}
            {{- end }}
          {{- end }}

          # Expose services
          {{- range gets "/services/default/*" }}
          {{- $service := json .Value }}
          {{- if and ($service.metadata.annotations) ($service.metadata.annotations.external_type) }}
          {{- if eq $service.metadata.annotations.external_type "http" }}
            {{ template "httpservice" $service }}
          {{- end }}
          {{- end }}
          {{- end }}

          # Expose services for statefulsets
          {{- range gets "/statefulsets/default/*" }}
          {{- $statefulset := json .Value }}
          {{- if and ($statefulset.metadata.annotations) ($statefulset.metadata.annotations.external_type) }}
          {{- if eq $statefulset.metadata.annotations.external_type "http" }}
            {{ $endpoints := print "/endpoints/default/" $statefulset.spec.serviceName }}
            {{- range $i, $subset := (json (getv $endpoints)).subsets }}
            {{- range $j, $addr := $subset.addresses }}
            {{- $name := split $addr.hostname "-" }}
            {{- $name := index $name 0 }}
            {{- if eq $statefulset.metadata.name $name }}
          backend http_backend_{{$addr.hostname}}_{{$statefulset.spec.serviceName}}
            balance roundrobin
            option httpclose
            option forwardfor
            server {{$addr.hostname}}_{{$statefulset.spec.serviceName}} {{$addr.ip}}:{{$statefulset.metadata.annotations.external_port}}
            {{- end }}
            {{- end }}
            {{- end }}
          {{- end }}
          {{- end }}
          {{- end }}

# TODO: Migrate to https
# This will be migrated when we can do wildcard certs with Let's Encrypt
# Until then, we need to keep 80 to handle review deploys.
- name: deploy http proxy in dev
  when: cluster_name in ['dev', 'staging']
  kubernetes:
    api_endpoint: 127.0.0.1:8001
    insecure: true
    state: apply
    inline_data: |
      apiVersion: extensions/v1beta1
      kind: DaemonSet
      metadata:
        name: proxy-internal-http
      spec:
        selector:
          matchLabels:
            app: proxy-internal-http
        template:
          metadata:
            name: proxy-internal-http
            annotations:
              prometheus.io/scrape: "true"
              prometheus.io/port: "9201"
            labels:
              app: proxy-internal-http
          spec:
            nodeSelector:
              role: proxy
            hostNetwork: true
            containers:
            - name: proxy
              image: registry.docker.sjc.aristanetworks.com:5000/k8s/proxy
              imagePullPolicy: Always
              args:
                - -backend
                - kubernetes
              ports:
                - name: https
                  containerPort: 80
                  hostPort: 80
              volumeMounts:
              - name: haproxy-conf-internal-http
                mountPath: /etc/confd/templates/haproxy.cfg.tmpl
                subPath: haproxy.cfg.tmpl
              - name: haproxy-conf-internal-http
                mountPath: /etc/confd/conf.d/haproxy.toml
                subPath: haproxy.toml
            - name: catchall-service
              image: nginx:1.13.5
              volumeMounts:
              - name: nginx-catchall-config
                mountPath: /etc/nginx/nginx.conf
                subPath: nginx.conf
            volumes:
            - name: nginx-catchall-config
              configMap:
                name: nginx-catchall-config
            - name: haproxy-conf-internal-http
              configMap:
                name: haproxy-conf-internal-http
