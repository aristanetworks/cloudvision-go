# Copyright (c) 2017 Arista Networks, Inc.  All rights reserved.
# Arista Networks, Inc. Confidential and Proprietary.
# Subject to Arista Networks, Inc.'s EULA.
# FOR INTERNAL USE ONLY. NOT FOR DISTRIBUTION.

---

- name: create am-data volumes
  kubernetes:
    api_endpoint: 127.0.0.1:8001
    insecure: true
    inline_data: |
      apiVersion: v1
      kind: PersistentVolume
      metadata:
        name: "am-data-{{ item | replace('.', '-') }}"
        annotations:
          "volume.alpha.kubernetes.io/node-affinity": '{
            "requiredDuringSchedulingIgnoredDuringExecution": {
              "nodeSelectorTerms": [
                { "matchExpressions": [
                  { "key": "kubernetes.io/hostname",
                    "operator": "In",
                    "values": [ "{{ item }}" ]
                  }
                ]}
               ]}
              }'
      spec:
        capacity:
          # TODO: Determine how much we want here.
          storage: 1Ti
        accessModes:
        - ReadWriteOnce
        persistentVolumeReclaimPolicy: Retain
        storageClassName: am-data
        local:
          path: {{ hostvars[item]["alertmanager_data_dir"] }}
  with_items: "{{ groups.alertmanager }}"

- name: deploy alertmanager service
  kubernetes:
    api_endpoint: 127.0.0.1:8001
    insecure: true
    state: present
    inline_data: |
      apiVersion: v1
      kind: Service
      metadata:
        name: alertmanager
        labels:
          app: alertmanager
        annotations:
          external_type: 'http'
          prometheus.io/scrape: 'true'
      spec:
        type: LoadBalancer
        ports:
        - name: web
          port: 9093
        selector:
          app: alertmanager

- name: deploy alertmanager config
  kubernetes:
    api_endpoint: 127.0.0.1:8001
    insecure: true
    state: present
    inline_data: |
      #jinja2: block_start_string: '[%', block_end_string: '%]', variable_start_string: '[[', variable_end_string: ']]'
      apiVersion: v1
      kind: ConfigMap
      metadata:
        name: alertmanager
      data:
        config.yml: |
          global:
            # The smarthost and SMTP sender used for mail notifications.
            smtp_smarthost: '[[ smtp_server ]]:25'
            smtp_from: 'alertmanager@arista.com'
            # smtp_auth_username: 'alertmanager'
            # smtp_auth_password: '1234'
            # Slack API URL
            slack_api_url: '[[ slack_api_url ]]'

          # The directory from which notification templates are read.
          # templates:
          # - '/etc/alertmanager/template/*.tmpl'

          # The root route on which each incoming alert enters.
          route:
            # The labels by which incoming alerts are grouped together. For example,
            # multiple alerts coming in for cluster=A and alertname=LatencyHigh would
            # be batched into a single group.
            group_by: ['alertname', 'app', 'instance']

            # When a new group of alerts is created by an incoming alert, wait at
            # least 'group_wait' to send the initial notification.
            # This way ensures that you get multiple alerts for the same group that start
            # firing shortly after another are batched together on the first
            # notification.
            group_wait: 3s

            # When the first notification was sent, wait 'group_interval' to send a batch
            # of new alerts that started firing for that group.
            group_interval: 1m

            # If an alert has successfully been sent, wait 'repeat_interval' to
            # resend them.
            repeat_interval: 3h

            # A default receiver
            receiver: email-alert

            # All the above attributes are inherited by all child routes and can
            # overwritten on each.

            # The child route trees.
            routes:
            - match:
                severity: critical
              receiver: email-alert

          # Inhibition rules allow to mute a set of alerts given that another alert is
          # firing.
          # We use this to mute any warning-level notifications if the same alert is
          # already critical.
          inhibit_rules:
          - source_match:
              severity: 'critical'
            target_match:
              severity: 'warning'
            # Apply inhibition if the alertname is the same.
            equal: ['alertname', 'app', 'instance']


          receivers:
          - name: email-alert
            email_configs:
            - to: '[[cluster_name]]-alert@arista.com'
          - name: slack-notifications
            slack_configs:
            - channel: '#[[cluster_name]]-alert'
              text: 'Instances: {{ range .Alerts }}{{ .Labels.instance }} {{ end }}'

- name: deploy alertmanager server
  kubernetes:
    api_endpoint: 127.0.0.1:8001
    insecure: true
    state: apply
    inline_data: |
      apiVersion: apps/v1beta1
      kind: StatefulSet
      metadata:
        name: alertmanager
      spec:
        serviceName: alertmanager
        replicas: 1
        template:
          metadata:
            labels:
              app: alertmanager
          spec:
            containers:
            - name: alertmanager
              args:
                - -config.file=/etc/alertmanager/config.yml
                - -storage.path=/am-data
                - -web.external-url=http://alertmanager.{{ k8s_domain }}
              image: prom/alertmanager:v0.8.0
              ports:
              - name: web
                containerPort: 9093
              volumeMounts:
              - name: config
                mountPath: /etc/alertmanager
              - name: data
                mountPath: /am-data
            securityContext:
              runAsUser: 1000
              fsGroup: 1000
            volumes:
            - name: config
              configMap:
                name: alertmanager
        volumeClaimTemplates:
        - metadata:
            name: data
          spec:
            accessModes: [ "ReadWriteOnce" ]
            storageClassName: am-data
            resources:
              requests:
                # TODO: Determine how much we want here.
                storage: 1Ti
