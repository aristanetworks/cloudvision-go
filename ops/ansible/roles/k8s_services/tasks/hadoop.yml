# Copyright (c) 2017 Arista Networks, Inc.  All rights reserved.
# Arista Networks, Inc. Confidential and Proprietary.
# Subject to Arista Networks, Inc.'s EULA.
# FOR INTERNAL USE ONLY. NOT FOR DISTRIBUTION.

---

- name: Deploy hadoop service
  kubernetes:
    api_endpoint: 127.0.0.1:8001
    insecure: true
    inline_data: |
      apiVersion: v1
      kind: Service
      metadata:
        name: hadoop
      spec:
        ports:
          - port: 1234
            name: placeholder
        clusterIP: None
        selector:
          app: hadoop

# JOURNALNODES

- name: Create journalnode-data volumes
  kubernetes:
    api_endpoint: 127.0.0.1:8001
    insecure: true
    inline_data: |
      apiVersion: v1
      kind: PersistentVolume
      metadata:
        name: "journalnode-data-{{ item | replace('.', '-') }}"
        annotations:
          "volume.alpha.kubernetes.io/node-affinity": '{
            "requiredDuringSchedulingIgnoredDuringExecution": {
              "nodeSelectorTerms": [
                { "matchExpressions": [
                  { "key": "kubernetes.io/hostname",
                    "operator": "In",
                    "values": [ "{{ item }}" ]
                  }
                ]}
               ]}
              }'
      spec:
        capacity:
          storage: 1Gi
        accessModes:
        - ReadWriteOnce
        persistentVolumeReclaimPolicy: Retain
        storageClassName: journalnode-data
        local:
          path: "{{ hostvars[item]['hadoop_journalnode_dir'] }}"
  with_items: "{{ groups.journalnodes }}"

- name: Deploy journalnode statefulset
  kubernetes:
    api_endpoint: 127.0.0.1:8001
    insecure: true
    inline_data: |
      apiVersion: apps/v1beta1
      kind: StatefulSet
      metadata:
        name: journalnode
      spec:
        serviceName: hadoop
        replicas: {{ groups.journalnodes | length }}
        podManagementPolicy: "Parallel"
        template:
          metadata:
            labels:
              app: hadoop
              hadoop: journalnode
          spec:
            containers:
            - name: server
              image: registry.docker.sjc.aristanetworks.com:5000/aeris/k8s-hadoop
              imagePullPolicy: Always
              args: ["./bin/hdfs", "journalnode"]
              env:
              - name: HADOOP_LOGFILE
                value: "journalnode.log"
              ports:
              - containerPort: 8485
              - containerPort: 8480
              - containerPort: 8481
              volumeMounts:
              - name: data
                subPath: data
                mountPath: /data
              - name: data
                subPath: logs
                mountPath: /logs
        volumeClaimTemplates:
        - metadata:
            name: data
          spec:
            accessModes: [ "ReadWriteOnce" ]
            storageClassName: journalnode-data
            resources:
              requests:
                storage: 1Gi

# NAMENODES

- name: Create namenode-data volumes
  kubernetes:
    api_endpoint: 127.0.0.1:8001
    insecure: true
    inline_data: |
      apiVersion: v1
      kind: PersistentVolume
      metadata:
        name: "namenode-data-{{ item | replace('.', '-') }}"
        annotations:
          "volume.alpha.kubernetes.io/node-affinity": '{
            "requiredDuringSchedulingIgnoredDuringExecution": {
              "nodeSelectorTerms": [
                { "matchExpressions": [
                  { "key": "kubernetes.io/hostname",
                    "operator": "In",
                    "values": [ "{{ item }}" ]
                  }
                ]}
               ]}
              }'
      spec:
        capacity:
          storage: 1Gi
        accessModes:
        - ReadWriteOnce
        persistentVolumeReclaimPolicy: Retain
        storageClassName: namenode-data
        local:
          # TODO: create only one volume for now since
          # we can only mount one local volume to a pod
          # should be fixed in k8s 1.9
          path: "{{ hostvars[item]['hadoop_namenode_dirs'][0] }}"
  with_items: "{{ groups.namenodes }}"


- name: Deploy namenode statefulset
  kubernetes:
    api_endpoint: 127.0.0.1:8001
    state: apply
    insecure: true
    inline_data: |
      apiVersion: apps/v1beta1
      kind: StatefulSet
      metadata:
        name: namenode
        annotations:
          external_type: http
          external_port: "50070"
      spec:
        serviceName: hadoop
        replicas: {{ groups.namenodes | length }}
        podManagementPolicy: "Parallel"
        template:
          metadata:
            labels:
              app: hadoop
              hadoop: namenode
          spec:
            containers:
            - name: server
              image: registry.docker.sjc.aristanetworks.com:5000/aeris/k8s-hadoop
              imagePullPolicy: Always
              args: ["./bin/hdfs", "namenode"]
              env:
              - name: HADOOP_LOGFILE
                value: "namenode.log"
              ports:
              - containerPort: 8020
              - containerPort: 50070
              volumeMounts:
              - name: data1
                subPath: data
                mountPath: /data1
              - name: data2
                subPath: data
                mountPath: /data2
              - name: data1
                subPath: logs
                mountPath: /logs
            - name: zkfc
              image: registry.docker.sjc.aristanetworks.com:5000/aeris/k8s-hadoop
              imagePullPolicy: Always
              args: ["./bin/hdfs", "zkfc"]
              env:
              - name: HADOOP_LOGFILE
                value: "zkfc.log"
              volumeMounts:
              - name: data1
                subPath: logs
                mountPath: /logs
            volumes:
            # TODO: make it a pvc when we can mount multiple local volumes to pods
            - name: data2
              hostPath:
                path: /data8/namenode
        volumeClaimTemplates:
        - metadata:
            name: data1
          spec:
            accessModes: [ "ReadWriteOnce" ]
            storageClassName: namenode-data
            resources:
              requests:
                storage: 1Gi

# DATANODES

- name: Create datanode-data volumes
  kubernetes:
    api_endpoint: 127.0.0.1:8001
    insecure: true
    inline_data: |
      apiVersion: v1
      kind: PersistentVolume
      metadata:
        name: "datanode-data-{{ item | replace('.', '-') }}"
        annotations:
          "volume.alpha.kubernetes.io/node-affinity": '{
            "requiredDuringSchedulingIgnoredDuringExecution": {
              "nodeSelectorTerms": [
                { "matchExpressions": [
                  { "key": "kubernetes.io/hostname",
                    "operator": "In",
                    "values": [ "{{ item }}" ]
                  }
                ]}
               ]}
              }'
      spec:
        capacity:
          storage: 2Ti
        accessModes:
        - ReadWriteOnce
        persistentVolumeReclaimPolicy: Retain
        storageClassName: datanode-data
        local:
          # TODO: create only one volume for now since
          # we can only mount one local volume to a pod
          # should be fixed in k8s 1.9
          path: "{{ hostvars[item]['hadoop_datanode_dirs'][0] }}"
  with_items: "{{ groups.datanodes }}"


- name: Deploy datanode statefulset
  kubernetes:
    api_endpoint: 127.0.0.1:8001
    insecure: true
    inline_data: |
      apiVersion: apps/v1beta1
      kind: StatefulSet
      metadata:
        name: datanode
      spec:
        serviceName: hadoop
        replicas: {{ groups.datanodes | length }}
        podManagementPolicy: "Parallel"
        template:
          metadata:
            labels:
              app: hadoop
              hadoop: datanode
          spec:
            containers:
            - name: server
              image: registry.docker.sjc.aristanetworks.com:5000/aeris/k8s-hadoop
              imagePullPolicy: Always
              args: ["./bin/hdfs", "datanode", "-Ddfs.datanode.data.dir=file:///data1,file:///data2,file:///data3,file:///data4"]
              env:
              - name: HADOOP_LOGFILE
                value: "datanode.log"
              ports:
              - containerPort: 50010
              - containerPort: 50075
              - containerPort: 50020
              volumeMounts:
              - name: data1
                subPath: data
                mountPath: /data1
              - name: data2
                subPath: data
                mountPath: /data2
              - name: data3
                subPath: data
                mountPath: /data3
              - name: data4
                subPath: data
                mountPath: /data4
              - name: data1
                subPath: logs
                mountPath: /logs
              - name: var-lib-hadoop
                mountPath: /var/lib/hadoop
            volumes:
            # TODO: make it a pvc when we can mount multiple local volumes to pods
            - name: data2
              hostPath:
                path: /data6/dfs
            - name: data3
              hostPath:
                path: /data7/dfs
            - name: data4
              hostPath:
                path: /data8/dfs
            - name: var-lib-hadoop
              hostPath:
                path: /var/lib/hadoop
        volumeClaimTemplates:
        - metadata:
            name: data1
          spec:
            accessModes: [ "ReadWriteOnce" ]
            storageClassName: datanode-data
            resources:
              requests:
                storage: 2Ti

